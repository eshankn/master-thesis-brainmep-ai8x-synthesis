---

arch: energy_profiling
dataset: chbmit_singlech_32samples_patient_5_leave_out_seizure_1

# Define layer parameters in order of the layer sequence

# --------------------------------------------------------------------------------
#                                     Variant 1
# --------------------------------------------------------------------------------
layers:
  # Layer 0: Block 1 Conv1d 1
  - name: block1_conv1d_bn_relu_1
    processors: 0x0000.0000.0000.0001
    out_offset: 0x4000
    data_format: CHW
    operation: Conv1d
    activate: ReLU
    kernel_size: 4
    stride: 1
    pad: 2
    in_channels: 1
    out_channels: 4
  # Layer 1: Block 1 MaxPool1d 1
  - name: block1_maxpool1d_1
    processors: 0x0000.0000.0000.000f
    out_offset: 0
    operation: None
    max_pool: 8
    pool_stride: 8
  # Layer 2: Block 2 Conv1d 1
  - name: block2_conv1d_bn_relu_1
    processors: 0x0000.0000.0000.000f
    out_offset: 0x4000
    operation: Conv1d
    activate: ReLU
    kernel_size: 4
    stride: 1
    pad: 2
    in_channels: 4
    out_channels: 4
  # Layer 3: Block 2 MaxPool1d 1
  - name: block2_maxpool1d_1
    processors: 0x0000.0000.0000.000f
    out_offset: 0
    operation: None
    max_pool: 8
    pool_stride: 8
#  # Layer 3: Block 1 Conv1d 3
#  - name: block1_conv1d_bn_relu_3
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 4
#    out_channels: 4
#  # Layer 1: Block 1 AvgPool1d 1
#  - name: block1_avgpool1d_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: None
#    avg_pool: 16
#    pool_stride: 16
#  # Layer 1: Block 1 AvgPool1d 2
#  - name: block1_avgpool1d_2
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0x4000
#    operation: None
#    avg_pool: 4
#    pool_stride: 4
  # Layer 4: Block 6 Dense
  - name: block6_dense
    processors: 0x0000.0000.0000.000f
    out_offset: 0x4000
    operation: Linear
    activate: None
    flatten: true
    output: true
    output_width: 32

# --------------------------------------------------------------------------------
#                                     Variant 2
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 1
#  # Layer 1: Block 1 Conv1d 2
#  - name: block1_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.0010
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 1
#  # Layer 2: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.0100
#    out_offset: 0
#    operation: None
#    max_pool: 3
#    pool_stride: 3
#  # Layer 3: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.1000
#    output_processors: 0x0000.0000.0003.0000
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                     Variant 3
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0001.0000
#    out_offset: 0
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 1
#  # Layer 1: Block 1 Conv1d 2
#  - name: block1_conv1d_bn_relu_2
#    processors: 0x0000.0000.0010.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 1
#  # Layer 2: Block 1 Conv1d 3
#  - name: block1_conv1d_bn_relu_3
#    processors: 0x0000.0000.0100.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 1
#  # Layer 3: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0001.0000.0000
#    out_offset: 0
#    operation: None
#    max_pool: 3
#    pool_stride: 3
#  # Layer 4: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0010.0000.0000
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                     Variant 4
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001   # 0x0000.0000.0001.0000
#    out_offset: 0x4000
#    data_format: HWC
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 Conv1d 2
#  - name: block1_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.1111   # 0x0000.0000.0010.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 4
#    out_channels: 4
#  # Layer 2: Block 1 Conv1d 3
#  - name: block1_conv1d_bn_relu_3
#    processors: 0x0000.0000.0000.1111   # 0x0000.0001.0000.0000
#    out_offset: 0x4000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 4
#    out_channels: 4
#  # Layer 3: Block 1 Conv1d 4
#  - name: block1_conv1d_bn_relu_4
#    processors: 0x0000.0000.0000.1111   # 0x0001.0000.0000.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 4
#    out_channels: 4
#  # Layer 4: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.000f   # 0x0000.0000.0000.0001
#    out_offset: 0x4000
#    operation: None
#    max_pool: 12
#    pool_stride: 12
#  # Layer 5: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.000f   # 0x0000.0000.0000.0010
#    output_processors: 0x0000.0000.0000.0003
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                     Variant 6
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001   # 0x0000.0000.0001.0000
#    out_offset: 0x4000
#    data_format: HWC
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 1
#    out_channels: 2
##  # Layer 6: Block 1 MaxPool1d 1
##  - name: block1_maxpool1d_1
##    processors: 0x0000.0000.0000.000f   # 0x0000.0000.0000.0001
##    out_offset: 0
##    operation: None
##    max_pool: 4
##    pool_stride: 4
#  # Layer 1: Block 1 Conv1d 2
#  - name: block1_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.0003   # 0x0000.0000.0010.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 2
#    out_channels: 2
#  # Layer 2: Block 1 Conv1d 3
#  - name: block1_conv1d_bn_relu_3
#    processors: 0x0000.0000.0000.0003   # 0x0000.0001.0000.0000
#    out_offset: 0x4000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 2
#    out_channels: 2
#  # Layer 3: Block 1 Conv1d 4
#  - name: block1_conv1d_bn_relu_4
#    processors: 0x0000.0000.0000.0003   # 0x0001.0000.0000.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 2
#    out_channels: 2
#  # Layer 4: Block 1 Conv1d 5
#  - name: block1_conv1d_bn_relu_5
#    processors: 0x0000.0000.0000.0003   # 0x0001.0000.0000.0000
#    out_offset: 0x4000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 2
#    out_channels: 2
#  # Layer 5: Block 1 Conv1d 6
#  - name: block1_conv1d_bn_relu_6
#    processors: 0x0000.0000.0000.0003   # 0x0001.0000.0000.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 2
#    out_channels: 2
#  # Layer 6: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_2
#    processors: 0x0000.0000.0000.0003   # 0x0000.0000.0000.0001
#    out_offset: 0x4000
#    operation: None
#    max_pool: 12
#    pool_stride: 12
#  # Layer 7: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.0003   # 0x0000.0000.0000.0010
#    output_processors: 0x0000.0000.0006.0000
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------

#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 3
#    stride: 1
#    pad: 1
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 2: Block 1 MaxPool1d 2
#  - name: block1_maxpool1d_2
#    processors: 0x0000.0000.0000.0f00
#    out_offset: 0
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 3: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.f000
#    out_offset: 0
#    operation: None
#    avg_pool: 16
#    pool_stride: 1
#  # Layer 4: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32
