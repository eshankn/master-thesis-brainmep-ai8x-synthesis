---

arch: epidenet_b
dataset: chbmit_singlech_768samples_patient_5_leave_out_seizure_1

# Define layer parameters in order of the layer sequence

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 1.1
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 3
#    stride: 1
#    pad: 1
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 2: Block 1 MaxPool1d 2
#  - name: block1_maxpool1d_2
#    processors: 0x0000.0000.0000.0f00
#    out_offset: 0
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 3: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.f000
#    out_offset: 0
#    operation: None
#    avg_pool: 16
#    pool_stride: 1
#  # Layer 4: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 1.2
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 4
#    stride: 1
#    pad: 2
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 Conv1d 2
#  - name: block1_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 4
#    out_channels: 4
#  # Layer 2: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.0f00
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 3: Block 1 MaxPool1d 2
#  - name: block1_maxpool1d_2
#    processors: 0x0000.0000.0000.f000
#    out_offset: 0
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 4: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: None
#    avg_pool: 16
#    pool_stride: 1
#  # Layer 5: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 2.1
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 4
#    stride: 1
#    pad: 2
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 Conv1d 2
#  - name: block1_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.00f0
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 0
#    in_channels: 4
#    out_channels: 4
#  # Layer 2: Block 1 MaxPool1d
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.0f00
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 3: Block 2 Conv1d 1
#  - name: block2_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.f000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 9
#    stride: 1
#    pad: 2
#    in_channels: 4
#    out_channels: 16
#  # Layer 4: Block 2 Conv1d 2
#  - name: block2_conv1d_bn_relu_2
#    processors: 0x0000.0000.ffff.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 8
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 5: Block 2 Conv1d 3
#  - name: block2_conv1d_bn_relu_3
#    processors: 0x0000.ffff.0000.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 3
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 6: Block 2 Conv1d 4
#  - name: block2_conv1d_bn_relu_4
#    processors: 0xffff.0000.0000.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 3
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 7: Block 2 Conv1d 5
#  - name: block2_conv1d_bn_relu_5
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 3
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 8: Block 2 Conv1d 6
#  - name: block2_conv1d_bn_relu_6
#    processors: 0x0000.0000.ffff.0000
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 2
#    stride: 1
#    pad: 1
#    in_channels: 16
#    out_channels: 16
#  # Layer 9: Block 2 MaxPool1d
#  - name: block2_maxpool1d_1
#    processors: 0x0000.ffff.0000.0000
#    out_offset: 0x2000
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 10: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.ffff.0000.0000
#    out_offset: 0
#    operation: None
#    avg_pool: 16
#    pool_stride: 1
#  # Layer 11: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.ffff.0000.0000
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 2.2
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0x2000
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 4
#    stride: 1
#    pad: 2
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 2: Block 2 Conv1d 1
#  - name: block2_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0x2000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 9
#    stride: 1
#    pad: 2
#    in_channels: 4
#    out_channels: 16
#  # Layer 3: Block 2 Conv1d 2
#  - name: block2_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 8
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 4: Block 2 MaxPool1d 1
#  - name: block2_maxpool1d_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x2000
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 5: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: None
#    avg_pool: 14
#    pool_stride: 1
#  # Layer 6: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x2000
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 2.3
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0x2000
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 4
#    stride: 1
#    pad: 2
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 2: Block 2 Conv1d 1
#  - name: block2_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0x2000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 9
#    stride: 1
#    pad: 2
#    in_channels: 4
#    out_channels: 16
#  # Layer 3: Block 2 MaxPool1d 1
#  - name: block2_maxpool1d_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 4: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x2000
#    operation: None
#    avg_pool: 15
#    pool_stride: 1
#  # Layer 5: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 3.1
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0x2000
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 4
#    stride: 1
#    pad: 2
#    in_channels: 1
#    out_channels: 4
#  # Layer 1: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
#  # Layer 2: Block 2 Conv1d 1
#  - name: block2_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0x2000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 9
#    stride: 1
#    pad: 2
#    in_channels: 4
#    out_channels: 16
#  # Layer 3: Block 2 Conv1d 2
#  - name: block2_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 8
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 4: Block 2 MaxPool1d 1
#  - name: block2_maxpool1d_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x2000
#    operation: None
#    max_pool: 4
#    pool_stride: 4
#  # Layer 5: Block 3 Conv1d 1
#  - name: block3_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 8
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
#  # Layer 6: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x2000
#    operation: None
#    avg_pool: 11
#    pool_stride: 1
#  # Layer 7: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 3.1 new
# --------------------------------------------------------------------------------
#layers:
#  # Layer 0: Block 1 Conv1d 1
#  - name: block1_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.0001
#    out_offset: 0x4000
#    data_format: CHW
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 4
#    stride: 1
#    pad: 2
#    in_channels: 1
#    out_channels: 4
##    quantization: 8
#  # Layer 1: Block 1 MaxPool1d 1
#  - name: block1_maxpool1d_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0
#    operation: None
#    max_pool: 8
#    pool_stride: 8
##    quantization: 4
#  # Layer 2: Block 2 Conv1d 1
#  - name: block2_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.000f
#    out_offset: 0x4000
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 9
#    stride: 1
#    pad: 2
#    in_channels: 4
#    out_channels: 16
##    quantization: 4
#  # Layer 3: Block 2 Conv1d 2
#  - name: block2_conv1d_bn_relu_2
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 8
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
##    quantization: 4
#  # Layer 6: Block 2 MaxPool1d 1
#  - name: block2_maxpool1d_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x4000
#    operation: None
#    max_pool: 4
#    pool_stride: 4
##    quantization: 4
#  # Layer 7: Block 3 Conv1d 1
#  - name: block3_conv1d_bn_relu_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: Conv1d
#    activate: ReLU
#    kernel_size: 8
#    stride: 1
#    pad: 2
#    in_channels: 16
#    out_channels: 16
##    quantization: 4
#  # Layer 8: Block 3 MaxPool1d 1
#  - name: block3_maxpool1d_1
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x4000
#    operation: None
#    max_pool: 4
#    pool_stride: 4
##    quantization: 4
#  # Layer 9: Block 5 AvgPool1d
#  - name: block5_avgpool1d
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0
#    operation: None
#    avg_pool: 4
#    pool_stride: 1
##    quantization: 4
#  # Layer 10: Block 6 Dense
#  - name: block6_dense
#    processors: 0x0000.0000.0000.ffff
#    out_offset: 0x4000
#    operation: Linear
#    activate: None
#    flatten: true
#    output: true
#    output_width: 32

# --------------------------------------------------------------------------------
#                                 EpiDeNet Variant 3.2
# --------------------------------------------------------------------------------
layers:
  # Layer 0: Block 1 Conv1d 1
  - name: block1_conv1d_bn_relu_1
    processors: 0x0000.0000.0000.0001
    out_offset: 0x4000
    data_format: CHW
    operation: Conv1d
    activate: ReLU
    kernel_size: 4
    stride: 1
    pad: 2
    in_channels: 1
    out_channels: 4
#    quantization: 8
  # Layer 1: Block 1 MaxPool1d 1
  - name: block1_maxpool1d_1
    processors: 0x0000.0000.0000.000f
    out_offset: 0
    operation: None
    max_pool: 8
    pool_stride: 8
#    quantization: 4
  # Layer 2: Block 2 Conv1d 1
  - name: block2_conv1d_bn_relu_1
    processors: 0x0000.0000.0000.000f
    out_offset: 0x4000
    operation: Conv1d
    activate: ReLU
    kernel_size: 5
    stride: 1
    pad: 2
    in_channels: 4
    out_channels: 16
#    quantization: 4
  # Layer 3: Block 2 Conv1d 2
  - name: block2_conv1d_bn_relu_2
    processors: 0x0000.0000.0000.ffff
    out_offset: 0
    operation: Conv1d
    activate: ReLU
    kernel_size: 5
    stride: 1
    pad: 2
    in_channels: 16
    out_channels: 16
#    quantization: 4
  # Layer 4: Block 2 Conv1d 3
  - name: block2_conv1d_bn_relu_3
    processors: 0x0000.0000.0000.ffff
    out_offset: 0x4000
    operation: Conv1d
    activate: ReLU
    kernel_size: 5
    stride: 1
    pad: 2
    in_channels: 16
    out_channels: 16
#    quantization: 4
  # Layer 5: Block 2 Conv1d 4
  - name: block2_conv1d_bn_relu_4
    processors: 0x0000.0000.0000.ffff
    out_offset: 0
    operation: Conv1d
    activate: ReLU
    kernel_size: 4
    stride: 1
    pad: 2
    in_channels: 16
    out_channels: 16
#    quantization: 4
  # Layer 6: Block 2 MaxPool1d 1
  - name: block2_maxpool1d_1
    processors: 0x0000.0000.0000.ffff
    out_offset: 0x4000
    operation: None
    max_pool: 4
    pool_stride: 4
#    quantization: 4
  # Layer 7: Block 3 Conv1d 1
  - name: block3_conv1d_bn_relu_1
    processors: 0x0000.0000.0000.ffff
    out_offset: 0
    operation: Conv1d
    activate: ReLU
    kernel_size: 8
    stride: 1
    pad: 2
    in_channels: 16
    out_channels: 16
#    quantization: 4
  # Layer 8: Block 3 MaxPool1d 1
  - name: block3_maxpool1d_1
    processors: 0x0000.0000.0000.ffff
    out_offset: 0x4000
    operation: None
    max_pool: 4
    pool_stride: 4
#    quantization: 4
  # Layer 9: Block 5 AvgPool1d
  - name: block5_avgpool1d
    processors: 0x0000.0000.0000.ffff
    out_offset: 0
    operation: None
    avg_pool: 5
    pool_stride: 1
#    quantization: 4
  # Layer 10: Block 6 Dense
  - name: block6_dense
    processors: 0x0000.0000.0000.ffff
    out_offset: 0x4000
    operation: Linear
    activate: None
    flatten: true
    output: true
    output_width: 32
